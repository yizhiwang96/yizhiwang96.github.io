<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0030)https://pengsongyou.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yizhi Wang</title>
  
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE">	
  <meta name="viewport" content="‚Äúwidth=800‚Äù">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">

  <style type="text/css">
    @import url(http://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300,100italic,100);
      /* Color scheme stolen from Sergey Karayev */
      a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th,tr,p,a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
      }
      strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
      }
      heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
      }
      papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight:500;
      }
      name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
      }
      .one
      {
      width: 160px;
      height: 140px;
      position: relative;
      }
      .two
      {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
      }
      .fade {
       transition: opacity .2s ease-in-out;
       -moz-transition: opacity .2s ease-in-out;
       -webkit-transition: opacity .2s ease-in-out;
      }
      span.highlight {
          background-color: #ffffd0;
      }
    </style>
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tbody><tr>
        <td width="67%" valign="middle">
          <p align="center">
            <name>Yizhi Wang (ÁéãÈÄ∏‰πã)</name>
            <!--<br>
            ruthfong at robots dot ox dot ac dot uk-->
          </p>

          <p>
            I am a Postdoc Fellow working with Professor <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a> in <a href="https://gruvi.ca/">GrUVi Lab</a> at Simon Fraser University (SFU), Canada. I am now working on 3D Shape Reconstruction and Content Creation.
          </p>
          <p>
            I received my PhD degree from Peking University, where I worked on Computer Vision and Computer Graphics. During my PhD, I was focused on applying deep generative models to analyze and synthesize 2D geometric data (e.g., glyph, font, and layout). I was supervised by Prof. <a href="https://www.icst.pku.edu.cn/zlian/">Zhouhui Lian</a> and Prof. <a href="https://scholar.google.com/citations?user=Ox427jAAAAAJ&hl=en&oi=ao">Jianguo Xiao</a>.
          </p>
              
          <p align="center">
            <a href="mailto:wangyizhi@pku.edu.cn">Email</a> &nbsp/&nbsp
            <a href="data/yizhi_wang_s_CV_2022_0622.pdf">CV</a> &nbsp/&nbsp
            <a href="https://scholar.google.com/citations?user=0S3NCzUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
            <a href="https://github.com/yizhiwang96/">Github</a>
          </p>
          </td>

          <td width="33%">
            <a href="images/yizhi_wang_s_selfie_phd.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yizhi_wang_s_selfie_phd.jpeg" class="hoverZoomLink"></a>
          </td>
        </tr>
        </tbody></table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td>
              <heading>News</heading>
                <ul>
                  <li><strong>06/2023</strong> Check how <a href="https://christinalj.com/">Christina</a> uses <a href="https://yizhiwang96.github.io/deepvecfont_homepage/">DeepVecFont</a> in her art-design project (<a href="https://www.youtube.com/watch?v=ar_kMCQ4DTI&t=403s&ab_channel=christinaissleepy">Video</a> and <a href="https://christinalj.com/crystal-type">Blog</a>)! </li>
                  <li><strong>05/2023</strong> Introducing <a href="https://bright-project01.github.io/">BRIGHT</a>, a new feature representation method, which can generate images with more accurate structures and less distortions. </li>
		</li></div>
                </ul>
            </td>
          </tr>
          </tbody>
        </table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have strong interests in shape synthesis and analysis. Specifically, my researching projects cover the following topics: 3D Reconstruction, Font Generation, Layout Generation, Scene Text (Character) Recognition and Detection, Font Recognition, etc. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>

	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()" bgcolor="#ffffd0">
            <td width="25%">
              <div class="one">
                <img src='images/aro_net.png' width="190">
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="">
                <papertitle>ARO-Net: Learning Implicit Fields from Anchored Radial Observations</papertitle>
              </a>
              <br>
              <strong>Yizhi Wang*</strong>,
              <a href="">Zeyu Huang*</a>,
	      <a href="https://faculty.runi.ac.il/arik/site/index.asp">Ariel Shamir</a>,
	      <a href="http://vcc.tech/~huihuang">Hui Huang</a>,
	      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>,
	      <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>
              <br>
              <em>CVPR</em>, 2023  
              <br>
							<a href="https://github.com/yizhiwang96/ARO-Net">project page</a> / 
							<a href="https://arxiv.org/abs/2212.10275">arXiv</a> / 
							
							<a href="https://github.com/yizhiwang96/ARO-Net">code</a> /
		    					<a href="https://www.youtube.com/watch?v=RVoOkgbi9lk&ab_channel=YizhiWang">video</a>
              <p></p>
              <p>A novel shape encoding for learning neural field representation of shapes that is category-agnostic and generalizable amid significant shape variations.</p>
            </td>
          </tr>

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td width="25%">
              <div class="one">
                <img src='images/bright.jpg' width="190">
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="https://bright-project01.github.io/">
                <papertitle>BRIGHT: Bi-level Feature Representation of Image Collections using Groups of Hash Tables</papertitle>
              </a>
              <br>
	      <a href="https://santisy.github.io/">Dingdong Yang</a>,
              <strong>Yizhi Wang</strong>,  
   
              <a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a>,
	      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
              <br>
              <em>Arxiv</em>, 2023  
              <br>
							<a href="https://bright-project01.github.io/">project page</a> / 
							<a href="https://arxiv.org/abs/2305.18601">arXiv</a> / 
							
							<a href="">code</a>		
              <p></p>
              <p>A bi-levelfeature representation for an imagecollection, consisting of a per-image latent space on top of a multi-scale feature grid space.</p>
            </td>
          </tr>	
	
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td width="25%">
              <div class="one">
                <video  width=118% height=118% muted autoplay loop>
                <source src="images/ds_fusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="https://ds-fusion.github.io/">
                <papertitle>DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion</papertitle>
              </a>
              <br>
	      <a href="http://mtanveer.com/">Maham Tanveer</a>,
              <strong>Yizhi Wang</strong>,  
   
              <a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a>,
	      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
              <br>
              <em>Arxiv</em>, 2023  
              <br>
							<a href="https://ds-fusion.github.io/">project page</a> / 
							<a href="https://arxiv.org/abs/2303.09604">arXiv</a> / 
							
							<a href="">code</a>		
              <p></p>
              <p>Automatically generate an artistic typography by stylizing one or more letter fonts to visually convey the semantics of an input word.</p>
            </td>
          </tr>	

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td width="25%">
              <div class="one">
                <video  width=118% height=118% muted autoplay loop>
                <source src="images/dvf_v2_2.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="">
                <papertitle>DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality</papertitle>
              </a>
              <br>
	      <a href="">Yuqing Wang</a>,
              <strong>Yizhi Wang</strong>,  
	      <a href="https://yulonghui.github.io/">Longhui Yu</a>,
	      <a href="https://scholar.google.com/citations?user=HBp_nuAAAAAJ&hl=zh-CN">Yuesheng Zhu</a>,
	      <a href="https://www.icst.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>CVPR</em>, 2023  
              <br>
							<a href="">project page</a> / 
							<a href="https://arxiv.org/abs/2303.14585">arXiv</a> / 
							
							<a href="https://github.com/yizhiwang96/deepvecfont-v2">code</a>		
              <p></p>
              <p>Employing Transformers and a relaxed representation for higher-quality vector font generation (support Chinese glyphs).</p>
            </td>
          </tr>
	
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td width="25%">
              <div class="one">
                <img src='images/cvpr22_textlogolayout.svg' width="190">
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="https://github.com/yizhiwang96/TextLogoLayout">
                <papertitle>Aesthetic Text Logo Synthesis via Content-aware Layout Inferring</papertitle>
              </a>
              <br>
              <strong>Yizhi Wang</strong>,
              <a href="https://github.com/TrickyGo">Guo Pu</a>,
              <a href="https://whluo.github.io/">Wenhan Luo</a>,
              <a href="">Yexin Wang</a>,
              <a href="https://scholar.google.com/citations?user=ctLbu3IAAAAJ&hl=en&oi=ao">Pengfei Xiong</a>,
              <a href="https://scholar.google.com/citations?user=uQg2t8EAAAAJ&hl=en&oi=ao">Hongwen Kang</a>,
              <a href="https://www.icst.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>CVPR</em>, 2022  
              <br>
							<a href="https://github.com/yizhiwang96/TextLogoLayout">project page</a> / 
							<a href="">arXiv</a> / 
							
							<a href="https://github.com/yizhiwang96/TextLogoLayout">code</a>
              <p></p>
              <p>A content-aware layout generation network which takes element images and their corresponding content (such as texts) as input and synthesizes aesthetic layouts for them automatically.</p>
            </td>
          </tr>

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()" bgcolor="#ffffd0">
            <td width="25%">
              <div class="one">
                <video  width=118% height=118% muted autoplay loop>
                <source src="images/tog21_deepvecfont.mp4" type="video/mp4">
                </video>
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="https://yizhiwang96.github.io/deepvecfont_homepage/">
                <papertitle>DeepVecFont: Synthesizing High-quality Vector Fonts via Dual-modality Learning</papertitle>
              </a>
              <br>
              <strong>Yizhi Wang</strong>,
              <a href="https://www.icst.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>ACM Transactions on Graphics (SIGGRAPH Asia 2021 Technical Paper)</em>, 2021  
              <br>
							<a href="https://yizhiwang96.github.io/deepvecfont_homepage/">project page</a> / 
							<a href="https://arxiv.org/abs/2110.06688">arXiv</a> / 
							
							<a href="https://github.com/yizhiwang96/deepvecfont">code</a>		
              <p></p>
              <p>Directly synthesize vector fonts via dual-modality learning and differentiable rasterization (rendering), instead of vectorizing synthesized glyph images by rule-based methods.</p>
            </td>
          </tr>

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()" bgcolor="#ffffd0">
            <td width="25%">
              <div class="one">
                <video  width=118% height=118% muted autoplay loop>
                <source src="images/tog20_attr2font_2.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="https://hologerry.github.io/Attr2Font/">
                <papertitle>Attribute2Font: Creating Fonts You Want From Attributes</papertitle>
              </a>
              <br>
              <strong>Yizhi Wang*</strong>,  
              <a href="https://yuegao.me/">Yue Gao*</a>,
              <a href="https://www.icst.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>ACM Transactions on Graphics (SIGGRAPH 2020 Technical Paper)</em>, 2020  
              <br>
							<a href="https://yuegao.me/Attr2Font/">project page</a> / 
							<a href="https://arxiv.org/abs/2005.07865">arXiv</a> / 
							
							<a href="https://github.com/hologerry/Attr2Font">code</a>		
              <p></p>
              <p>Automatically synthesize fonts according to user-specified attributes (such as italic, serif, cursive, and angularity) and their corresponding values.</p>
            </td>
          </tr>

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td width="25%">
              <div class="one">
                <img src='images/mm20_efifstr.svg' width="190">
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="https://actasidiot.github.io/EFIFSTR">
                <papertitle>Exploring Font-independent Features for Scene Text Recognition</papertitle>
              </a>
              <br>
              <strong>Yizhi Wang</strong>,
              <a href="https://www.icst.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>ACM Multimedia</em>, 2020  
              <br>
							<a href="https://actasidiot.github.io/EFIFSTR">project page</a> / 
							<a href="https://arxiv.org/abs/2009.07447">arXiv</a> / 
							
							<a href="https://github.com/Actasidiot/EFIFSTR">code</a>		
              <p></p>
              <p>Address the challenge of font variance in STR and propose a font-independent feature representation method to increase the robustness of STR models.</p>
            </td>
          </tr>
					
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td width="25%">
              <div class="one">
                <img src='images/mmm18_fontrec.png' width="190">
              </div>
            </td>
            <td valign="top" width="75%">
							<a href="https://www.wict.pku.edu.cn/zlian/frwild/">
                <papertitle> Font Recognition in Natural Images via Transfer Learning</papertitle>
              </a>
              <br>
              <strong>Yizhi Wang</strong>,
              <a href="https://www.icst.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>MMM</em>, 2018  
              <br>
							<a href="https://www.wict.pku.edu.cn/zlian/frwild/">project page</a> / 
							<a href="https://www.wict.pku.edu.cn/zlian/docs/20181024110641005904.pdf">paper</a> /		
              <a href="https://drive.google.com/drive/folders/1Q3Go8aU9EPFfsQZCbqoZdASbhqngvkXc?usp=sharing">dataset</a>
              <p></p>
              <p>Accurately recognize the font styles of texts in natural images by proposing a novel method based on deep learning and transfer learning.</p>
            </td>
          </tr>
          


        </tbody></table>
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td>
              <heading>Invited Talks</heading>
                <ul>
                  <li><strong>07/2022</strong> I was invited to give <a href="https://www.bilibili.com/video/BV16Z4y1a7hg">a talk</a> (in Chinese) by CSIG (China Society of Image and Graphics). </li>

                </li></div>
                </ul>
            </td>
          </tr>
          </tbody>
        </table>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td width="100%" valign="center">
              Merit Student, Peking University. 2018, 2021
              <br>
              Excellent Student, Wangxuan Insitute of Peking University. 2020, 2021
              <br>
              CETC The 14TH Research Institute Glarun Scholarship, Peking University. 2018
              <br>
              Excellent Award, The 17th Programming Contest of Peking University. 2018
            </td>
          </tr>
        </tbody></table>
        <table width="50%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="50%" valign="center"></td>       
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=l3MhaRO61ZZJ-JzbbrMHHIDHgCHMg5-1lw2H1HHU3wA&cl=ffffff&w=a"></script>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                template adapted from <a href="https://pengsongyou.github.io/">Songyou</a> and <a href="https://jonbarron.info/">Jon</a>.
              </p>
            </td>
          </tr>
        </tbody></table>        
      </td>
    </tr>
  </table>
</body>

</html>
